#!/bin/bash

#SBATCH --job-name=metaline
#SBATCH --qos=gp_bscls
#SBATCH --account=bsc40
#SBATCH --output=/gpfs/projects/bsc40/current/dmajer/metaline-prepare-greasy-array-job/test_output/jobnm_%j.out # Output file, where
#SBATCH --error=/gpfs/projects/bsc40/current/dmajer/metaline-prepare-greasy-array-job/test_output/jobnm_%j.err # File where the error is written

#SBATCH --array=1-14 # The number of jobs in the array
#SBATCH --ntasks=14 # The number of parallel tasks
#SBATCH --tasks-per-node=2
#SBATCH --cpus-per-task=110 # Number of CPUs per run task
#SBATCH --constraint=highmem  # Use this only if your the database is too big and this constrainst is available in your cluster.
#SBATCH --time=24:00:00  # 1 day.

module load greasy
# Print the task id
$(sed -n "${SLURM_ARRAY_TASK_ID}p" /gpfs/projects/bsc40/current/dmajer/metaline-prepare-greasy-array-job/test_output/list_greasy.txt)
